{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Using ART on Traffic Sign Classifier"}, {"metadata": {}, "cell_type": "markdown", "source": "## Introduction:"}, {"metadata": {}, "cell_type": "markdown", "source": "ART is a library developed by \"IBM Research\" which is dedicated to adversarial machine learning. Its purpose is to allow rapid crafting and analysis of attacks and defense methods for machine learning models. ART provides an implementation for many state-of-the-art methods for attacking and defending classifiers. \n\n- ART Demo: https://art-demo.mybluemix.net\n- ART Blog: https://www.ibm.com/blogs/research/2018/04/ai-adversarial-robustness-toolbox/\n- ART Github: https://github.com/IBM/adversarial-robustness-toolbox\n\n\nIn this notebook, you will work with Adversarial Robustness Toolbox (ART) and try to implement an adversarial attack and its defense on a trained model.\nYou will work with a model trained on German Traffic Signs dataset and try to get the model to misclassify a stop sign.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Dataset Citation\n\nJ. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1453\u20131460. 2011. <br>\n\n@inproceedings{Stallkamp-IJCNN-2011,<br>\n    author = {Johannes Stallkamp and Marc Schlipsing and Jan Salmen and Christian Igel},<br>\n    booktitle = {IEEE International Joint Conference on Neural Networks},<br>\n    title = {The {G}erman {T}raffic {S}ign {R}ecognition {B}enchmark: A multi-class classification competition},<br>\n    year = {2011},<br>\n    pages = {1453--1460}<br>\n}"}, {"metadata": {}, "cell_type": "markdown", "source": "## Objective:"}, {"metadata": {}, "cell_type": "markdown", "source": "Upon completing the lab, you will learn:\n\n- How to load a Tensorflow trained model\n- How to create an ART classifier object using the loaded model\n- How to perfom an adversarial attack\n- How to perfom a defense to make sure manipulated images can still be classified correctly"}, {"metadata": {}, "cell_type": "markdown", "source": "## Library Versions:\n\n- ART version 0.9.0\n- Tensorflow version 1.15"}, {"metadata": {}, "cell_type": "markdown", "source": "## Instructions:"}, {"metadata": {}, "cell_type": "markdown", "source": "Please follow the steps in notebook below."}, {"metadata": {}, "cell_type": "markdown", "source": "---\n## Step 1: Load required libraries\n"}, {"metadata": {}, "cell_type": "code", "source": "!pip install wget\nimport wget \n\nlink_to_data = 'https://github.com/bleonardb3/Think2020/raw/master/Lab-3/Adversarial-Robustness-Toolbox-0.9.0.tar.gz'\n\n!rm Adversarial-Robustness-Toolbox-0.9.0.tar.gz\n\nwget.download(link_to_data)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install 'Adversarial-Robustness-Toolbox-0.9.0.tar.gz'\n\n!pip install opencv-python\n!pip install numpy", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Here we load all the libraries needed to run this notebook."}, {"metadata": {}, "cell_type": "code", "source": "# common import for all cells\nSEED=202\n\n# standard libs\nimport pickle\nimport csv\nfrom timeit import default_timer as timer\nimport os\nfrom os.path import join, abspath, expanduser\nimport sys\n\n\n#visualisation\n%matplotlib inline\n\nimport matplotlib.pyplot as plt \nfrom IPython.display import Image\nfrom IPython.display import display\n\n# numerical Libraries \nimport cv2\nimport math\n\nimport random\nimport numpy as np\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nimport tensorflow as tf\ntf.set_random_seed(SEED)\nfrom tensorflow import keras\nfrom tensorflow.python.training import moving_averages\nfrom tensorflow.contrib.framework import add_model_variable\nsess = tf.InteractiveSession()\n\nimport keras\nimport keras.backend as k\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.preprocessing import image\n\n# ART Libraries\nfrom art.classifiers import TFClassifier\n#from art.utils import load_dataset\n#from art.attacks.fast_gradient import FastGradientMethod\nfrom art.attacks import ProjectedGradientDescent\n#from art.detection.detector import BinaryInputDetector", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "---\n## Step 2: Load The Data"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section we define a function to load our Traffic Sign dataset which is in \"Pickle\" file format.\n\nThe pickled data is a dictionary with 4 key/value pairs:\n\n- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. These coordinates assume the original image. The pickled data, containes resized versions (32 x 32) of these images."}, {"metadata": {}, "cell_type": "markdown", "source": "## INSERT OBJECT STORAGE CREDENTIALS\n----\n\n\n#### a. Please insert the IBM Cloud Object Storage credentials in the cell below."}, {"metadata": {}, "cell_type": "code", "source": "credentials={\n#   'apikey': \"Bk7zOKRYr9qVkZyFmZQUGDH0bZ-a-PBargE1nGKqedOT\",\n#   'endpoints': \"https://s3-api.us-geo.objectstorage.service.networklayer.com\",\n#   'resource_instance_id': \"crn:v1:bluemix:public:cloud-object-storage:global:a/745879b6e1314c84b790bf90e21994b3:49656519-f026-4560-8b46-a29457efa1dc::\",\n#   'BUCKET': 'watsonstudiolabs-donotdelete-pr-ptznjdche7dyeb',\n   'apikey': \"XsWGp-u7xhQmdcJFQm2-EPhWPNU1boOlHo8VP0n8W6uT\",\n   'endpoints': \"https://s3-api.us-geo.objectstorage.service.networklayer.com\",\n   'resource_instance_id': \"crn:v1:bluemix:public:cloud-object-storage:global:a/6111bdc91ee705970b7d8b4f0188cbfd:838529f7-eaf2-4f3b-9ec1-d5ac228a1bcd::\",\n   'BUCKET': 'adversarialrobustnesstoolkit-donotdelete-pr-gfodsg09fvjys0',\n   'FILE_TRAIN': 'train.p',\n   'FILE_VALID': 'valid.p',\n   'FILE_TEST': 'test.p',\n   'FILE_SIGN': 'signnames.csv',\n   'FILE_SIGN_ALL': 'signnames_all.jpg',\n   'MODEL_DATA': 'final.ckpt.data-00000-of-00001',\n   'MODEL_CHECKPOINT': 'checkpoint',\n   'MODEL_CKPT_INDEX': 'final.ckpt.index',\n   'MODEL_META': 'final.ckpt.meta',\n   'FILE_STOP': 'stop_001.png',\n   'FILE_DIAG_1': '001.png',\n   'FILE_DIAG_2': '002.png'\n}\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### b. Following cell contains code for initiating a connection to the cloud object storage."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_botocore.client import Config\nimport ibm_boto3\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['apikey'],\n    ibm_service_instance_id=credentials['resource_instance_id'],\n    ibm_auth_endpoint='https://iam.ng.bluemix.net/oidc/token',\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['endpoints'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### c. Following cell contains code to download the dataset and image files required for the lab."}, {"metadata": {}, "cell_type": "code", "source": "\n!mkdir checkpoints\n!mkdir checkpoints/final.ckpt\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_TRAIN'],Filename='training_file')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_VALID'],Filename='valid_file')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_TEST'],Filename='testing_file')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_SIGN'],Filename='classname_file')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_SIGN_ALL'],Filename='signnames_all_file')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_META'],Filename='checkpoints/final.ckpt.meta')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_DATA'],Filename='checkpoints/final.ckpt.data-00000-of-00001')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_CHECKPOINT'],Filename='checkpoints/checkpoint')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_CKPT_INDEX'],Filename='checkpoints/final.ckpt.index')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_STOP'],Filename='stop_001.png')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_DIAG_1'],Filename='diagram1.png')\nres=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_DIAG_2'],Filename='diagram2.png')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### d. Following cell contains code for loading the trainset and test set, and then Splitting them in x_train, y_train, x_test and y_test."}, {"metadata": {}, "cell_type": "code", "source": "# Load pickled data  \n\ndef load_data(): \n    training_file  = 'training_file'\n    testing_file   = 'testing_file'\n    classname_file = 'classname_file'\n\n    classnames = []\n    with open(classname_file) as _f:\n        rows = csv.reader(_f, delimiter=',')\n        next(rows, None)  # skip the headers\n        for i, row in enumerate(rows):\n            assert(i==int(row[0]))\n            classnames.append(row[1])\n \n    with open(training_file, mode='rb') as f:\n        train = pickle.load(f)\n    with open(testing_file, mode='rb') as f:\n        test = pickle.load(f)\n\n    X_train, y_train = train['features'], train['labels']\n    X_test, y_test   = test['features'], test['labels']\n    \n    \n    X_train  = X_train.astype(np.float32)\n    y_train  = y_train.astype(np.int32)\n    X_test   = X_test.astype(np.float32)\n    y_test   = y_test.astype(np.int32)\n    \n    return  classnames, X_train, y_train, X_test, y_test \n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "---\n\n## Step 3: Dataset Summary & Exploration\n\nIn the visualization section you will see representation of each class as well as number of samples in each class.\n"}, {"metadata": {}, "cell_type": "code", "source": "classnames, X_train, y_train, X_test, y_test = load_data() \n \n# Number of training examples \nnum_train = len(X_train)\n\n# Number of testing examples.\nnum_test = len(X_test)\n\n# the shape of an traffic sign image\n_, height, width, channel = X_train.shape\nimage_shape = (height, width, channel)\n\n# Number of unique classes/labels in the dataset\nnum_class = len(np.unique(y_train))\n\n\nprint(\"Number of training examples =\", num_train )\nprint(\"Number of testing examples =\", num_test )\nprint(\"Image data shape =\", image_shape)\nprint(\"Number of classes =\", num_class)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Visualization"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section, you will visualize the dataset to see what different classes look like.\nAlso, you demonstrate the sample count of each class.\n\nThis Section might take couple of seconds to minutes to complete (based on how powerful the system you are working on is)"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# Some functions to help us draw graphs\n\ndef get_label_image(c): \n    img=cv2.imread('signnames_all_file',1)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    H, W, _ = img.shape\n    dH = H/7.\n    dW = W/7.105\n    y = c//7\n    x = c%7 \n    label_image = img[round(y*dH):round(y*dH+dH), round(x*dW):round(x*dW+dW),:]\n    label_image = cv2.resize(label_image, (0,0), fx=32./dW, fy=32./dH,)\n    return label_image\n\n\ndef insert_subimage(image, sub_image, y, x): \n    h, w, c = sub_image.shape\n    image[y:y+h, x:x+w, :]=sub_image \n    return image\n\n\n# Start Visualizing the dataset\n\ntrain_images, train_labels = X_train, y_train\n\n#results image\nnum_sample=10\nresults_image = 255.*np.ones(shape=(num_class*height,(num_sample+2+22)*width, channel),dtype=np.float32)\nfor c in range(num_class):\n    label_image = get_label_image(c)\n    insert_subimage(results_image, label_image, c*height, 0)\n\n    #make mean\n    idx = list(np.where(train_labels== c)[0])\n    mean_image = np.average(train_images[idx], axis=0)\n    insert_subimage(results_image, mean_image, c*height, width)\n\n    #make random sample\n    for n in range(num_sample):\n        sample_image = train_images[np.random.choice(idx)]\n        insert_subimage(results_image, sample_image, c*height, (2+n)*width)\n\n    #print summary\n    count=len(idx)\n    percentage = float(count)/float(len(train_images))\n    cv2.putText(results_image, '%02d:%-6s'%(c, classnames[c]), ((2+num_sample)*width, int((c+0.7)*height)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1)\n    cv2.putText(results_image, '[%4d]'%(count), ((2+num_sample+14)*width, int((c+0.7)*height)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1)\n    cv2.rectangle(results_image,((2+num_sample+16)*width, c*height),((2+num_sample+16)*width + round(percentage*3000), (c+1)*height),(0,0,255),-1)\n\n\nprint('** training data summary **')\nprint('\\t1st column: label(image)')\nprint('\\t2nd column: mean image')\nprint('\\tother column: example images')\nprint('\\tblack text: label')\nprint('\\tblue text: sample count for each class and histogram plot')\nplt.rcParams[\"figure.figsize\"] = (25,25)\nplt.imshow(results_image.astype(np.uint8))\nplt.axis('off') \nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "----\n\n## Step 4: Defining Model Architecture\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section we define our DNN layers and architecture.\nThis model is a modified Densenet architecture. [3]\n"}, {"metadata": {}, "cell_type": "code", "source": "##  global varaiables ##\nIS_TRAIN_PHASE = tf.placeholder(dtype=tf.bool, name='is_train_phase')\n\ndef conv2d(input, num_kernels=1, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', has_bias=True, name='conv'):\n\n    input_shape = input.get_shape().as_list()\n    assert len(input_shape)==4\n    C = input_shape[3]\n    H = kernel_size[0]\n    W = kernel_size[1]\n    K = num_kernels\n\n    ##[filter_height, filter_width, in_channels, out_channels]\n    w    = tf.get_variable(name=name+'_weight', shape=[H, W, C, K], initializer=tf.truncated_normal_initializer(stddev=0.1))\n    conv = tf.nn.conv2d(input, w, strides=stride, padding=padding, name=name)\n    if has_bias:\n        b = tf.get_variable(name=name + '_bias', shape=[K], initializer=tf.constant_initializer(0.0))\n        conv = conv+b\n\n    return conv\n\n\ndef relu(input, name='relu'):\n    act = tf.nn.relu(input, name=name)\n    return act\n\ndef prelu(input, name='prelu'):\n    alpha = tf.get_variable(name=name+'_alpha', shape=input.get_shape()[-1],\n                       #initializer=tf.constant_initializer(0.25),\n                        initializer=tf.random_uniform_initializer(minval=0.1, maxval=0.3),\n                        dtype=tf.float32)\n    pos = tf.nn.relu(input)\n    neg = alpha * (input - abs(input)) * 0.5\n\n    return pos + neg\n\n\n# very leaky relu\ndef vlrelu(input, alpha=0.25, name='vlrelu'): #  alpha between 0.1 to 0.5\n    act =tf.maximum(alpha*input,input)\n    return act\n\ndef maxpool(input, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', has_bias=True, name='max' ):\n    H = kernel_size[0]\n    W = kernel_size[1]\n    pool = tf.nn.max_pool(input, ksize=[1, H, W, 1], strides=stride, padding=padding, name=name)\n    return pool\n\ndef avgpool(input, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', has_bias=True, is_global_pool=False, name='avg'):\n\n    if is_global_pool==True:\n        input_shape = input.get_shape().as_list()\n        assert len(input_shape) == 4\n        H = input_shape[1]\n        W = input_shape[2]\n\n        pool = tf.nn.avg_pool(input, ksize=[1, H, W, 1], strides=[1,H,W,1], padding='VALID', name=name)\n        pool = flatten(pool)\n\n    else:\n        H = kernel_size[0]\n        W = kernel_size[1]\n        pool = tf.nn.avg_pool(input, ksize=[1, H, W, 1], strides=stride, padding=padding, name=name)\n\n    return pool\n\n\ndef dropout(input, keep=1.0, name='drop'):\n    #drop = tf.cond(IS_TRAIN_PHASE, lambda: tf.nn.dropout(input, keep), lambda: input)\n    drop = tf.cond(IS_TRAIN_PHASE,\n                   lambda: tf.nn.dropout(input, keep),\n                   lambda: tf.nn.dropout(input, 1))\n    return drop\n\n\ndef flatten(input, name='flat'):\n    input_shape = input.get_shape().as_list()        # list: [None, 9, 2]\n    dim   = np.prod(input_shape[1:])                 # dim = prod(9,2) = 18\n    flat  = tf.reshape(input, [-1, dim], name=name)  # -1 means \"all\"\n    return flat\n\ndef concat(input, name='cat'):\n    cat = tf.concat(axis=3, values=input, name=name)\n    return cat\n\n\n#https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.contrib.layers.batch_norm.md\n#http://www.bubufx.com/detail-1792794.html\ndef bn (input, decay=0.9, eps=1e-5, name='bn'):\n    with tf.variable_scope(name) as scope:\n        bn = tf.cond(IS_TRAIN_PHASE,\n            lambda: tf.contrib.layers.batch_norm(input,  decay=decay, epsilon=eps, center=True, scale=True,\n                              is_training=1,reuse=None,\n                              updates_collections=None, scope=scope),\n            lambda: tf.contrib.layers.batch_norm(input, decay=decay, epsilon=eps, center=True, scale=True,\n                              is_training=0, reuse=True,\n                              updates_collections=None, scope=scope))\n\n    return bn\n\n\n# basic building blocks\n\ndef conv2d_bn_relu(input, num_kernels=1, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', name='conv'):\n    with tf.variable_scope(name) as scope:\n        block = conv2d(input, num_kernels=num_kernels, kernel_size=kernel_size, stride=stride, padding=padding, has_bias=False)\n        block = bn(block)\n        block = relu(block)\n    return block\n\n\ndef bn_relu_conv2d (input, num_kernels=1, kernel_size=(1, 1), stride=[1, 1, 1, 1], padding='SAME', name='conv'):\n    with tf.variable_scope(name) as scope:\n        block = bn(input)\n        block = relu(block)\n        block = conv2d(block, num_kernels=num_kernels, kernel_size=kernel_size, stride=stride, padding=padding, has_bias=False)\n    return block\n\n\n\n# modified dense block from the paper [1] \"Densely Connected Convolutional Networks\" - Gao Huang, Zhuang Liu, Kilian Q. Weinberger, \n# Laurens van der Maaten, Arxiv 2016\n# Modification: \n#   1. the paper uses bn-relu-conv but we use conv-bn-relu\n#   2. the paper uses dropout inside the block but we shift the dropout outside the block see network construction later\ndef dense_block_cbr (input, num=1, num_kernels=1, kernel_size=(1, 1), drop=None, name='DENSE'):\n \n    block = input\n    for n in  range(num):\n        with tf.variable_scope(name+'_%d'%n) as scope:\n            conv = conv2d(block, num_kernels=num_kernels, kernel_size=kernel_size, stride=[1,1,1,1], padding='SAME', has_bias=False)\n            conv = bn(conv)\n            conv = relu(conv)\n\n            if drop is not None:\n                keep = (1 - drop) ** (1. / num)\n                conv = dropout(conv, keep=keep)\n\n            block = concat((block, conv))\n    return block\n\n# the loss \ndef l2_regulariser(decay):\n\n    variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    for v in variables:\n        name = v.name\n        if 'weight' in name:  #this is weight\n            l2 = decay * tf.nn.l2_loss(v)\n            tf.add_to_collection('losses', l2)\n        elif 'bias' in name:  #this is bias\n            pass\n        elif 'beta' in name:\n            pass\n        elif 'gamma' in name:\n            pass\n        elif 'moving_mean' in name:\n            pass\n        elif 'moving_variance' in name:\n            pass\n        elif 'moments' in name:\n            pass\n\n        else:\n            #pass\n            #raise Exception('unknown variable type: %s ?'%name)\n            pass\n\n    l2_loss = tf.add_n(tf.get_collection('losses'))\n    return l2_loss\n\n\ndef cross_entropy(logit, label, name='cross_entropy'):\n    label = tf.cast(label, tf.int64)\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=label), name=name)\n    return cross_entropy\n\n\ndef accuracy(prob, label, name='accuracy'):\n    correct_prediction = tf.equal(tf.argmax(prob, 1), tf.cast(label, tf.int64))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=name)\n    return accuracy\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# my densenet here!\n# the inference part (without loss)\n\ndef DenseNet_3( input_shape=(1,1,1), output_shape = (1)):\n\n    H, W, C   = input_shape\n    num_class = output_shape\n    input     = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')\n\n    #color preprocessing using conv net:\n    #see \"Systematic evaluation of CNN advances on the ImageNet\"-Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas, ARXIV 2016\n    # https://arxiv.org/abs/1606.02228\n    # we use learnable prelu (different from paper) and 3x3 onv\n    with tf.variable_scope('preprocess') as scope:\n        input = bn(input, name='b1')\n        input = conv2d(input, num_kernels=8, kernel_size=(3, 3), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c1')\n        input = prelu(input, name='r1')\n        input = conv2d(input, num_kernels=8, kernel_size=(1, 1), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c2')\n        input = prelu(input, name='r2')\n\n\n    with tf.variable_scope('block1') as scope:\n        block1 = conv2d_bn_relu(input, num_kernels=32, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME')\n        block1 = maxpool(block1, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')\n\n    # we use conv-bn-relu in DENSE block (different from paper)\n    # dropout is taken out of the block\n    with tf.variable_scope('block2') as scope:\n        block2 = dense_block_cbr(block1, num=4, num_kernels=16, kernel_size=(3, 3), drop=None)\n        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')\n\n    with tf.variable_scope('block3') as scope:\n        block3 = dense_block_cbr(block2, num=4, num_kernels=24, kernel_size=(3, 3), drop=None)\n        block3 = dropout(block3, keep=0.9)\n        block3 = maxpool(block3,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')\n\n    with tf.variable_scope('block4') as scope:\n        block4 = dense_block_cbr(block3, num=4, num_kernels=32, kernel_size=(3, 3), drop=None)\n        block4 = conv2d_bn_relu(block4, num_kernels=num_class, kernel_size=(1,1), stride=[1, 1, 1, 1], padding='SAME')\n        block4 = dropout(block4, keep=0.8)\n        block4 = avgpool(block4, is_global_pool=True)\n\n\n    logit = block4\n    return logit\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### An ilustration on what the network acually looks like (Visually)"}, {"metadata": {}, "cell_type": "code", "source": "img1=Image(filename=\"diagram1.png\")\nimg2=Image(filename=\"diagram2.png\")\ndisplay(img1, img2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "---\n## Step 5: Using ART to Attack & Defense"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will load the trained model and start working with ART. \n\n- You will first define a TFClassifier which is an ART object that will be used as the model when you are working with ART.\n- After defining the TFClassifier, you will try it on a clean (non-adversarial) image and see the prediction. \n- Then you will craft an Adversarial image using \"Projected Gradient Descent method\" and feed it to the model to see its prediction.\n- Finally, you will use a defense method called \"Feature Squeezing Defence\" on the adversarial image to neutralize it. Then you will feed it to the model and see the result."}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1. Defining model parameters and loading the trained model"}, {"metadata": {}, "cell_type": "code", "source": "adv_make_net = DenseNet_3\n\nadv_logit  = adv_make_net(input_shape =(32, 32, 3), output_shape=(num_class))\nadv_data   = tf.get_default_graph().get_tensor_by_name('input:0')\nadv_label  = tf.placeholder(dtype=tf.int32, shape=[None, 43])\nadv_prob   = tf.nn.softmax(adv_logit)\n\nadv_l2     = l2_regulariser(decay=0.0005)\nadv_loss   = cross_entropy(adv_logit, adv_label)\nadv_metric = accuracy(adv_prob, adv_label)\n\nadv_learning_rate = tf.placeholder(tf.float32, shape=[])\nadv_solver = tf.train.MomentumOptimizer(learning_rate=adv_learning_rate, momentum=0.9)\nadv_solver_step = adv_solver.minimize(adv_loss)\n\nadv_sess = tf.Session()\n\nadv_saver  = tf.train.Saver()\nadv_saver.restore(adv_sess, 'checkpoints/final.ckpt')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2. Defining TFClassifier\n\nTFClassifier is an ART object that containes the trained model."}, {"metadata": {}, "cell_type": "code", "source": "# classifier = TFClassifier(input_ph=adv_data, output = adv_logit, labels_ph=adv_label, train=adv_solver_step, loss=adv_loss, learning = IS_TRAIN_PHASE, sess=adv_sess, preprocessing= (0, 1))\nclassifier = TFClassifier(adv_data, adv_logit, output_ph=adv_label, train=adv_solver_step, loss=adv_loss, learning = IS_TRAIN_PHASE, sess=adv_sess, preprocessing= (0, 1))\n\nclassifier.set_learning_phase(train=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classifier)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.3. Loading a \"Clean\" test image and test the TFClassifier on it\n\nYou can see that it detected that by 100% confidence it is a stop sign --> Correctly classified the image."}, {"metadata": {}, "cell_type": "code", "source": "from keras.preprocessing import image\n\ninit_img = np.zeros(shape=(1,32,32,3),dtype=np.float32)\n\nimage_file = join('stop_001.png')\nimage_ = image.load_img(image_file, target_size=(32, 32))\ninit_img = image.img_to_array(image_)\nimg = init_img[None , ...]\n\npred = classifier.predict(img)\nlabel = np.argmax(pred, axis=1)[0]\nconfidence = pred[:,label][0]\nprint('Prediction:', classnames[label], '- confidence {0:.0f}%'.format(100*confidence))\nplt.figure(figsize=(5,5))\nplt.imshow(init_img / 255)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n# 5.4. Perform Attack\n\nHere we will use \"Projected Gradient Descent\" attack. The PGD attack is a white-box attack (meaning the attacker has access to the model gradients i.e. the attacker has a copy of your model\u2019s weights). This threat model gives the attacker much more power than black box attacks as they can specifically craft their attack to fool your model without having to rely on transfer attacks that often result in human-visible perturbations. PGD can be considered the most \u201ccomplete\u201d white-box adversary as it lifts any constraints on the amount of time and effort the attacker can put into finding the best attack.\n\nyou will perform following steps in this section:\n\n1- Generate an adversarial image using \"Projected Gradient Descent\"\n   - First generate a carefully crafted noise\n   - Then apply it to the original image to craft the adversarial image\n   \n2- Feed the Adversary to the classifier and evaluate it on the trained model\n\n3- Finally we visualize the images (original, the crafted noise, adversarial image and a representation of the class that was predicted based on the adversarial image)\n\nReference Link to research paper on the attack: https://arxiv.org/abs/1706.06083\n"}, {"metadata": {}, "cell_type": "code", "source": "adv = ProjectedGradientDescent(classifier, targeted=False, eps_step=1, eps=2, max_iter=50)\n\n# Generate attack image\nimg_adv = adv.generate(img)\n\n# Evaluate it on model\npred_adv = classifier.predict(img_adv)\nlabel_adv = np.argmax(pred_adv, axis=1)[0]\nconfidence_adv = pred_adv[:, label_adv][0]\nprint('Prediction:', classnames[label_adv], '- confidence {0:.0f}%'.format(100*confidence_adv) ) \n\nfig = plt.figure(figsize=(16,16))\n\na = fig.add_subplot(1, 4, 1)\na.set_title('Original Predicted Sign\\nwith {0:.0f}% Accuracy'.format(100*confidence), fontsize=12)\nimgplot = plt.imshow(img[0] / 255)\nimgplot.set_clim(0.0, 0.7)\n\na = fig.add_subplot(1, 4, 2)\na.set_title('+      Noise      ---->', fontsize=12)\nimgplot = plt.imshow(np.abs(img[0] - img_adv[0]) )\n\na = fig.add_subplot(1, 4, 3)\na.set_title('Adversarial Image  ---->', fontsize=12)\nimgplot = plt.imshow(img_adv[0] / 255)\n\na = fig.add_subplot(1, 4, 4)\na.set_title('Predicted Sign\\n with {0:.0f}% Accuracy'.format(100*confidence_adv), fontsize=12)\nim = get_label_image(int(label_adv))\nimL = cv2.resize(im, dsize=(32, 32), interpolation=cv2.INTER_LINEAR)\nimgL = np.float32(imL)\nimgplot = plt.imshow(imgL / 255)\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.5. Apply Defense\n\nIn this section You will apply a defense against the adversarial image. Here we assume that we already know the input is manipulated and we want to neutralize it in a way that the DNN can correctly classify it as \"Stop Sign\".\n\nYou will use \"Feature Squeezing\" defense to neutralize the adversarial image. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model\u2019s prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy.\n\nThis method is based on paper named \"Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks\" by Weilin Xu, David Evans, Yanjun Qi from University of Virginia. Link :   https://arxiv.org/abs/1704.01155"}, {"metadata": {}, "cell_type": "code", "source": "from art.defences import FeatureSqueezing\n\nfs = FeatureSqueezing(bit_depth=3, clip_values=(0, 255))\nimg_def = fs(img_adv)\npred_def = classifier.predict(img_def[0])\nlabel_def = np.argmax(pred_def, axis=1)[0]\nconfidence_def = pred_def[:, label_def][0]\nprint('Prediction:', classnames[label_def], '- confidence {0:.2f}%'.format(100*confidence_def))\n\nfig = plt.figure(figsize=(15,15))\n\na = fig.add_subplot(1, 3, 1)\nimgplot = plt.imshow(img[0] / 255)\nimgplot.set_clim(0.0, 0.7)\na.set_title('Original Image', fontsize = 12)\n\na = fig.add_subplot(1, 3, 2)\nimgplot = plt.imshow(img_adv[0] / 255)\na.set_title('Adversarial Image\\n  Misclassified as \"Ahead Only\"', fontsize=12)\n\na = fig.add_subplot(1, 3, 3)\nimgplot = plt.imshow(img_def[0][0] / 255)\nimgplot.set_clim(0.0, 0.7)\na.set_title('Cleaned Image\\n Correctly classified with {0:.0f}% Accuracy'.format(100*confidence_def), fontsize=12)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### References\n\n[1] The Traffic sign classifier uses German Traffic Sign dataset: http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset\n\n[2] The code to define and train the model was used from the code in https://github.com/hengck23-udacity/udacity-driverless-car-nd-p2\n\n[3] \"Densely Connected Convolutional Networks\" - Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten, Arxiv 2016"}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}