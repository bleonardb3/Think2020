{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Mitigating Bias Using AI Fairness 360\n",
    "## Using \"Adversarial Debiasing\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning models are increasingly used to inform high stakes decisions about people. Although machine learning, by its very nature, is always a form of statistical discrimination, the discrimination becomes objectionable when it places certain privileged groups at systematic advantage and certain unprivileged groups at systematic disadvantage. Biases in training data, due to either prejudice in labels or under-/over-sampling, yields models with unwanted bias.\n",
    "\n",
    "AI Fairness 360 is an open source toolkit developed by IBM Research, that can help you examine, report, and mitigate discrimination and bias in machine learning models throughout the AI application lifecycle. The goal of this lab is to introduce its bias detection functionalities and help mitigate the bias in the machine learning model using Adversarial Debiasing technique.\n",
    "\n",
    "For more information see links below:\n",
    "\n",
    "- AIF360 Demo: https://aif360.mybluemix.net\n",
    "- AIF360 GitHub: https://github.com/IBM/AIF360\n",
    "- AIF360 API Docs: https://aif360.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this notebook you will utilize AIF360 to detect and mitigate bias on Compas dataset which is used to assess the likelihood that a criminal deefendant will reoffend. \n",
    "\n",
    "Upon completing this lab you will learn:\n",
    "\n",
    "- How to load datasets from the toolkit package\n",
    "- Check the dataset for bias\n",
    "- Mitigate existing bias in using Adversarial Debiasing technique\n",
    "- Train on both original and corrected dataset and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Please follow the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Required Libraries and Functions\n",
    "Here we load all libraries required to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n",
    "\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# to plot graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from colorama import Fore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Importing COMPAS Dataset\n",
    "\n",
    "Across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant’s likelihood of becoming a recidivist – a term used to describe criminals who re-offend. One of the tools used is called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions) andf the dataset it uses is the Compas dataset which you will be working with in this lab. By analyzing this datset, it was found that black defendants were far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk, therfore, this dataset is biased.\n",
    "\n",
    "To perform bias detection and mitigation using AIF360, the toolkit needs to be tailored to the particular bias of interest. More specifically, it needs to know the attribute or attributes, called \"protected attributes\", that are of interest: in this dataset, \"race\" and \"sex\" are protected attributes.\n",
    "\n",
    "For purpose of this lab, we choose \"Race\" attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig = load_preproc_data_compas()\n",
    "\n",
    "privileged_groups = [{'race': 1}]\n",
    "unprivileged_groups = [{'race': 0}]\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape (Number of Rows , Number of Columns)\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Visualization\n",
    "\n",
    "In this step, you will see a visual analysis on the Compas Dataset. This cell plots out the risk score for two racial groups in focus: \"African-American\" and \"White-Caucasian\". These charts show that scores for white defendants were skewed toward lower-risk categories. Scores for black defendants were not. This leads to the beliefe that African-American defendants are a higher risk and may re-offend more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Import Compas data set as a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget \n",
    "import pandas as pd\n",
    "link_to_data = 'https://raw.githubusercontent.com/bleonardb3/Think2020/master/Lab-1/data/compas-scores-two-years.csv'\n",
    "\n",
    "# make sure no duplicates\n",
    "!rm  compas-scores-two-years.csv\n",
    "\n",
    "data_set = wget.download(link_to_data)\n",
    "\n",
    "print(data_set)\n",
    "\n",
    "compas_data_df = pd.read_csv(data_set, sep=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Visualize Compas Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_data = compas_data_df.loc[compas_data_df['race'] == \"African-American\"]\n",
    "wc_data = compas_data_df.loc[compas_data_df['race'] == \"Caucasian\"]\n",
    "\n",
    "af_data.hist(column=['decile_score.1'], bins=19, figsize=(10,5), xlabelsize=12, ylabelsize=12, grid = False)\n",
    "plt.xlabel(\"Risk Score\", fontsize=15)\n",
    "plt.ylabel(\"Count\",fontsize=15)\n",
    "plt.ylim([0, 700])\n",
    "plt.xlim([0.5,10.5])\n",
    "plt.xticks(np.arange(1, 11, step=1))\n",
    "plt.title(\"Caucasian Defendant's Risk Score\", fontsize = 18)\n",
    "\n",
    "wc_data.hist(column=['decile_score.1'], bins=19, figsize=(10,5), xlabelsize=12, ylabelsize=12, grid = False)\n",
    "plt.xlabel(\"Risk Score\", fontsize=15)\n",
    "plt.ylabel(\"Count\",fontsize=15)\n",
    "plt.ylim([0, 700])\n",
    "plt.xlim([0.5,10.5])\n",
    "plt.xticks(np.arange(1, 11, step=1))\n",
    "plt.title(\"Black Defendant's Risk Score\", fontsize = 18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Checking Original Dataset for Bias\n",
    "\n",
    "Now that we've identified the protected attribute and defined privileged and unprivileged values, we can use AIF360 to detect bias in the dataset. One simple test is to compare the percentage of favorable results for the privileged and unprivileged groups, subtracting the former percentage from the latter. A negative value indicates less favorable outcomes for the unprivileged groups. This is implemented in the method called mean_difference on the BinaryLabelDatasetMetric class. The code below performs this check and displays the output on both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Check for bias in dataset after scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "dataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\n",
    "dataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "display(Markdown(\"Note that scaling the dataset did not have an effect on the result.\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train on Original Dataset\n",
    "\n",
    "Here we train a Logistic Regression model on the original training data and obtain the accuracy. The original accuracy is necessary because we need to compare it to the accuracy obtained after bias mitigation - to make sure our model will continue to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)\n",
    "plain_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Classification Metric References **\n",
    "\n",
    "Classification metric refers to a quantification of unwanted bias in models. \n",
    "\n",
    "Moving forward with the lab, you will compute and compare several classification metrics as below. Please refer to this cell when comparing metric values. Each metric value should fall within a certain range for their corresponding metric. \n",
    "\n",
    "- #### Disparate Impact\n",
    "Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group. The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group. Fairness for this metric is between 0.8 and 1.2\n",
    "\n",
    "- #### Equal Opportunity Difference\n",
    "This metric is computed as the difference of true positive rates between the unprivileged and the privileged groups. The true positive rate is the ratio of true positives to the total number of actual positives for a given group. The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group. Fairness for this metric is between -0.1 and 0.1\n",
    "\n",
    "- #### Average Odds Difference\n",
    "Computed as average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups. The ideal value of this metric is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group. Fairness for this metric is between -0.1 and 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Classification Metrics of Plain Model - not Debiased\n",
    "\n",
    "Here note the results. The items in red are outside the bounds of a fair model => original model is biased, we need to apply bias mitigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n",
    "\n",
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_nodebiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "\n",
    "print(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "if 0.8 < classified_metric_nodebiasing_test.disparate_impact() < 1.2:\n",
    "    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "else:\n",
    "    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact() , Fore.RED + \"      *** Bias Detected\")\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.equal_opportunity_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "else:\n",
    "    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference(), Fore.RED + \"      *** Bias Detected\")\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.average_odds_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "else:\n",
    "    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference(), Fore.RED + \"      *** Bias Detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Adversarial Debiasing\n",
    "\n",
    "Adversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions [1]. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.\n",
    "\n",
    "In this cell, you will create a Debiased Model using Adversarial Debiasing and train it on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)\n",
    "debiased_model.fit(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Comparing Results\n",
    "\n",
    "This step calculates and prints out the result of the bias mitigation process. The lines printed in red show bias in the original model and the lines with green writing \"Fairness Achieved\" mean that the bias mitigation algorithm was successfuly applied fairness to the model and the classification metric results have moved closer to the aceptable range. In a best case scenario they will fall in the desired range. This means fairness is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the plain model to test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)\n",
    "\n",
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\n",
    "print(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\n",
    "print(Fore.RESET + \"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "\n",
    "if 0.8 < classified_metric_nodebiasing_test.disparate_impact() < 1.2:\n",
    "    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "else:\n",
    "    print(Fore.RED + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.equal_opportunity_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "else:\n",
    "    print(Fore.RED + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "\n",
    "if  -0.1 < classified_metric_nodebiasing_test.average_odds_difference() < 0.1:\n",
    "    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "else:\n",
    "    print(Fore.RED + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "\n",
    "# print(Fore.RESET + \"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact(), Fore.GREEN + \"    *** Fairness Achieved ***\")\n",
    "print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference(), Fore.GREEN + \"    *** Fairness Achieved ***\")\n",
    "print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference(), Fore.GREEN + \"    *** Fairness Achieved ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Visualizing Results\n",
    "\n",
    "What does the numbers above mean? This cell demostrates the result of the bias mitigation on graphs. The blue bar is the mitigated result, notice that it is closer to the desired range and in the base case scenario, it falls in the desired range. This means fairness is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_DI = classified_metric_nodebiasing_test.disparate_impact()\n",
    "mitigated_DI = classified_metric_debiasing_test.disparate_impact()\n",
    "\n",
    "orig_EOD = classified_metric_nodebiasing_test.equal_opportunity_difference()\n",
    "mitigated_EOD = classified_metric_debiasing_test.equal_opportunity_difference()\n",
    "\n",
    "orig_AOD = classified_metric_nodebiasing_test.average_odds_difference()\n",
    "mitigated_AOD = classified_metric_debiasing_test.average_odds_difference()\n",
    "\n",
    "N = 2\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.5       # the width of the bars: can also be len(x) sequence\n",
    "xtype = ('Original', 'Mitigated')\n",
    "ind = np.arange(len(xtype))\n",
    "\n",
    "# Disparate Impact\n",
    "\n",
    "values_DI = [orig_DI, mitigated_DI]\n",
    "# print(\"values_DI =  \", values_DI, end='\\t')\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "p1_DI = plt.bar(ind, values_DI, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black') #, yerr=menStd)\n",
    "plt.xticks(ind, xtype)\n",
    "plt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Disparate Impact', fontsize=15)\n",
    "plt.ylim([-0.4, 1.2])\n",
    "plt.yticks(np.arange(-0.4, 1.3, 0.1)) \n",
    "plt.axhspan(ymin = 0.8, ymax = 1.2, color='green', alpha=0.1)\n",
    "plt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n",
    "# plt.legend(xtype)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "# Equal Opprortunity Difference\n",
    "\n",
    "values_EOD = [orig_EOD, mitigated_EOD]\n",
    "# print(\"values_EOD = \", values_EOD, end='\\t')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "p1_EOD = plt.bar(ind, values_EOD, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black')\n",
    "plt.xticks(ind, xtype)\n",
    "plt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Equal Opportunity Difference', fontsize=15)\n",
    "plt.ylim([-0.4, 1.2])\n",
    "plt.yticks(np.arange(-0.4, 1.3, 0.1))\n",
    "plt.axhspan(ymin = -0.1, ymax = 0.1, color='green', alpha=0.1)\n",
    "plt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "# Average odds difference\n",
    "\n",
    "values_AOD = [orig_AOD, mitigated_AOD]\n",
    "# print(\"values_AOD = \", values_AOD)\n",
    "\n",
    "ax = plt.subplot(1,3,3)\n",
    "p1_AOD = plt.bar(ind, values_AOD, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black') #, yerr=menStd)\n",
    "plt.xticks(ind, xtype)\n",
    "plt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Average Odds Difference', fontsize=15)\n",
    "plt.ylim(-0.4, 1.2)\n",
    "plt.yticks(np.arange(-0.4, 1.3, 0.1))\n",
    "plt.axhspan(ymin = -0.1, ymax = 0.1, color='Green', alpha=0.1)\n",
    "plt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n",
    "plt.subplots_adjust(top=1, bottom=0.08, left=0.10, right=2.5, hspace=0.5, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \n",
    "AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
