{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Detecting and Mitigating Bias Using AI Fairness 360\n## Using \"Adversarial Debiasing\"\n---"}, {"metadata": {}, "cell_type": "markdown", "source": "## Introduction\n\n Machine learning models are increasingly used to inform high stakes decisions about people. Although machine learning, by its very nature, is always a form of statistical discrimination, the discrimination becomes objectionable when it places certain privileged groups at systematic advantage and certain unprivileged groups at systematic disadvantage. Biases in training data, due to either prejudice in labels or under-/over-sampling, yields models with unwanted bias.\n\nThe AI Fairness 360 Python package includes a comprehensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models. The AI Fairness 360 interactive demo provides a gentle introduction to the concepts and capabilities. The tutorials and other notebooks offer a deeper, data scientist-oriented introduction. The complete API is also available. \n\nFor more information see links below:\n\n- AIF360 Demo: https://aif360.mybluemix.net\n- AIF360 GitHub: https://github.com/IBM/AIF360\n- AIF360 API Docs: https://aif360.readthedocs.io/en/latest/"}, {"metadata": {}, "cell_type": "markdown", "source": "## Tutorial Objective\n\nIn this notebook you will utilize AIF360 to detect and mitigate bias on the Compas dataset which is used to assess the likelihood that a criminal defendant will re-offend. \n\nUpon completing this lab you will learn how to:\n\n- Load datasets from the toolkit package\n- Check the dataset for bias\n- Mitigate existing bias in using Adversarial Debiasing technique\n- Train on both original and corrected dataset and compare results"}, {"metadata": {}, "cell_type": "markdown", "source": "## Environment\nThis tutorial uses a Jupyter Notebook, an open-source web applicaiton that allows you to create and share documents that contain instructions as well as live code.\n\nThe Jupyter Notebooks we are using today is based on a Watson Studio environment, a set of open source packages that provide us with a standardized data analysis tools. At multiple points during the demo, we will important additional tools we need for specific steps:\n\nE.g. `import pandas as pd` -> to import the \"pandas\" tool for data manipulation.\n\nE.g. `!pip install wget` -> to install a tool \"wget\" to download data from external webpages.\n\nWatson Studio also contains a set of functionality that allows a user to pre-define a set of environments down to the package version level as well as define the hardware configurations available to certain users, allowing teams to easily standardize toolsets and resources. If needed, we can also connect our notebooks to GPUs, Apache Spark, and external clusters for higher performance."}, {"metadata": {}, "cell_type": "markdown", "source": "## Instructions\n\nPlease follow the steps below:"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1- Required Libraries and Functions\nHere we load the AI Fairness 360 library."}, {"metadata": {}, "cell_type": "code", "source": "!pip install aif360", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%matplotlib inline\n# Load all necessary packages\nimport sys\nimport pandas as pd\n\nfrom aif360.datasets import BinaryLabelDataset\nfrom aif360.datasets import CompasDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric\nfrom aif360.metrics import ClassificationMetric\nfrom aif360.metrics.utils import compute_boolean_conditioning_vector\n\nfrom aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n\nfrom aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom IPython.display import Markdown, display\n\n# to plot graphs\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import colors\nfrom matplotlib.ticker import PercentFormatter\n\nimport tensorflow as tf\n\nfrom colorama import Fore\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2- Importing COMPAS Dataset\n\nAcross the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant\u2019s likelihood of becoming a recidivist \u2013 a term used to describe criminals who re-offend. One of the tools used is called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions) and the dataset it uses is the Compas dataset which you will be working with in this lab. By analyzing this datset, it was found that African-American defendants were far more likely than Caucasian defendants to be incorrectly judged to be at a higher risk of recidivism, while Caucasian defendants were more likely than African-American defendants to be incorrectly flagged as low risk, therefore, this dataset is biased.\n\nTo perform bias detection and mitigation using AIF360, the toolkit needs to be tailored to the particular bias of interest. More specifically, it needs to know the attribute or attributes to track, called \"protected attributes\", that are of interest: in this dataset, \"race\" and \"sex\" are protected attributes.\n\nFor purpose of this lab, we choose the \"Race\" attribute.\n\nIn the code below, `dataset_orig` loads the COMPAS dataset. We then set numerical values for race for privileged and unprivileges groups."}, {"metadata": {}, "cell_type": "code", "source": "# Get the dataset and split into train and test\ndataset_orig = load_preproc_data_compas()\n\nprivileged_groups = [{'race': 1}]\nunprivileged_groups = [{'race': 0}]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Below, we split our dataset into a testing set and a training set. A training set is used to build our machine learning model. The test data set is used to assess the performance of our model."}, {"metadata": {}, "cell_type": "code", "source": "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n\n# print out some labels, names, etc.\ndisplay(Markdown(\"#### Training Dataset shape (Number of Rows , Number of Columns)\"))\nprint(dataset_orig_train.features.shape)\ndisplay(Markdown(\"#### Protected attribute names\"))\nprint(dataset_orig_train.protected_attribute_names)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.1. Data Visualization\n\nBefore we begin building our models, we want to understand the shape of our data. In this step, you will see a visual analysis on the Compas Dataset. This cell plots out the risk score for two racial groups in focus: \"African-American\" and \"White-Caucasian\". These charts show that scores for White-Caucasian defendants were skewed toward lower-risk categories. Scores for African American defendants were not. This leads to the belief that African-American defendants are a higher risk and may re-offend more frequently."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.1 Import Compas data set as a Pandas Dataframe"}, {"metadata": {}, "cell_type": "markdown", "source": "`import wget` pulls in the wget library to load data from the github link.\n\n`import pandas as pd` pulls in the Pandas library for data manipulation and analysis. \n\n`!rm compas-scores-two-years.csv` removes data that we may have pulled in on a previous run of this notebook."}, {"metadata": {}, "cell_type": "code", "source": "!pip install wget\nimport wget \nimport pandas as pd\nlink_to_data = 'https://raw.githubusercontent.com/bleonardb3/Think2020/master/Lab-1/data/compas-scores-two-years.csv'\n\n# make sure no duplicates\n!rm  compas-scores-two-years.csv\n\ndata_set = wget.download(link_to_data)\n\nprint(data_set)\n\ncompas_data_df = pd.read_csv(data_set, sep=',')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1.2 Visualize Compas Data "}, {"metadata": {}, "cell_type": "markdown", "source": "The code below uses plt to stand in for Matplotlib, a general purpose \"plotting\" library to create the graphs below. The series of lines below set graph parameters and then displays the graph."}, {"metadata": {}, "cell_type": "code", "source": "af_data = compas_data_df.loc[compas_data_df['race'] == \"African-American\"]\nwc_data = compas_data_df.loc[compas_data_df['race'] == \"Caucasian\"]\n\naf_data.hist(column=['decile_score.1'], bins=19, figsize=(10,5), xlabelsize=12, ylabelsize=12, grid = False)\nplt.xlabel(\"Risk Score\", fontsize=15)\nplt.ylabel(\"Count\",fontsize=15)\nplt.ylim([0, 700])\nplt.xlim([0.5,10.5])\nplt.xticks(np.arange(1, 11, step=1))\nplt.title(\"Caucasian Defendant's Risk Score\", fontsize = 18)\n\nwc_data.hist(column=['decile_score.1'], bins=19, figsize=(10,5), xlabelsize=12, ylabelsize=12, grid = False)\nplt.xlabel(\"Risk Score\", fontsize=15)\nplt.ylabel(\"Count\",fontsize=15)\nplt.ylim([0, 700])\nplt.xlim([0.5,10.5])\nplt.xticks(np.arange(1, 11, step=1))\nplt.title(\"African-American's Risk Score\", fontsize = 18)\n\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Checking Original Dataset for Bias\n\nNow that we've identified the protected attribute and defined privileged and unprivileged values, we can use AIF360 to detect bias in the dataset. One simple test is to compare the percentage of favorable results for the privileged and unprivileged groups, subtracting the former percentage from the latter. A negative value indicates less favorable outcomes for the unprivileged groups. Within AIF360, this is implemented in the method called mean_difference on the BinaryLabelDatasetMetric class. The code below performs mean difference calculation and displays the output on both train and test sets."}, {"metadata": {}, "cell_type": "code", "source": "# Metric for the original dataset\nmetric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\ndisplay(Markdown(\"#### Original training dataset\"))\nprint(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\nmetric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\nprint(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The negative output indicates less favorable outcomes for the unprivileged group."}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1. Check for bias in dataset after scaling the data\n\nScaling is a common data preparation technique to ensure that large numerical values do not overwhelm a smaller ones. In the code below, we test scaling our inputs to see if the differences between privileged and unprivileged group outcomes changes. Though we do not find a change in outcomes, this is a good practice to consider when pre-processing data for certain types of machine learning models."}, {"metadata": {}, "cell_type": "code", "source": "min_max_scaler = MaxAbsScaler()\ndataset_orig_train.features = min_max_scaler.fit_transform(dataset_orig_train.features)\ndataset_orig_test.features = min_max_scaler.transform(dataset_orig_test.features)\nmetric_scaled_train = BinaryLabelDatasetMetric(dataset_orig_train, \n                             unprivileged_groups=unprivileged_groups,\n                             privileged_groups=privileged_groups)\ndisplay(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\ndisplay(Markdown(\"Note that scaling the dataset did not have an effect on the result.\"))\nprint(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\nmetric_scaled_test = BinaryLabelDatasetMetric(dataset_orig_test, \n                             unprivileged_groups=unprivileged_groups,\n                             privileged_groups=privileged_groups)\nprint(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Train on Original Dataset\n\nHere we train a Logistic Regression model on the original training data and obtain the accuracy using TensorFlow.\n\nTensorFlow is an extremely popular machine learning library. See: https://www.tensorflow.org/\n\nObtaining the original accuracy here is necessary because we need to compare it to the accuracy obtained after bias mitigation - to make sure our model will continue to perform well.\n\n`sess = tf.Session()` sets TensorFlow to a session. The remaining code defines the parameters for our model and then trains it."}, {"metadata": {}, "cell_type": "code", "source": "sess = tf.Session()\nplain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n                          unprivileged_groups = unprivileged_groups,\n                          scope_name='plain_classifier',\n                          debias=False,\n                          sess=sess)\nplain_model.fit(dataset_orig_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## ** Classification Metric References **\n\nClassification metric refers to a quantification of unwanted bias in models. \n\nMoving forward with the lab, you will compute and compare several classification metrics as below. Please refer to this cell when comparing metric values. Each metric value should fall within a certain range for their corresponding metric. \n\n- #### Disparate Impact\nComputed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group. The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group. Fairness for this metric is between 0.8 and 1.2\n\n- #### Equal Opportunity Difference\nThis metric is computed as the difference of true positive rates between the unprivileged and the privileged groups. The true positive rate is the ratio of true positives to the total number of actual positives for a given group. The ideal value is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group. Fairness for this metric is between -0.1 and 0.1\n\n- #### Average Odds Difference\nComputed as average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups. The ideal value of this metric is 0. A value of < 0 implies higher benefit for the privileged group and a value > 0 implies higher benefit for the unprivileged group. Fairness for this metric is between -0.1 and 0.1"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1. Classification Metrics of Plain Model - not Debiased\n\nThe code below calculates and displays the results of the plain, non-debiased model for the classification metrics above (disparate impact, equal opportunity difference, and average odds difference)."}, {"metadata": {}, "cell_type": "code", "source": "# Apply the plain model to train and test data\ndataset_nodebiasing_train = plain_model.predict(dataset_orig_train)\ndataset_nodebiasing_test = plain_model.predict(dataset_orig_test)\n\n# Metrics for the dataset from plain model (without debiasing)\ndisplay(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\nmetric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\nprint(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n\nmetric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\nprint(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n\ndisplay(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\nclassified_metric_nodebiasing_test = ClassificationMetric(dataset_orig_test, \n                                                 dataset_nodebiasing_test,\n                                                 unprivileged_groups=unprivileged_groups,\n                                                 privileged_groups=privileged_groups)\nprint(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\nTPR = classified_metric_nodebiasing_test.true_positive_rate()\nTNR = classified_metric_nodebiasing_test.true_negative_rate()\nbal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n\nprint(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\nif 0.8 < classified_metric_nodebiasing_test.disparate_impact() < 1.2:\n    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\nelse:\n    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact() , Fore.RED + \"      *** Bias Detected\")\n\nif  -0.1 < classified_metric_nodebiasing_test.equal_opportunity_difference() < 0.1:\n    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\nelse:\n    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference(), Fore.RED + \"      *** Bias Detected\")\n\nif  -0.1 < classified_metric_nodebiasing_test.average_odds_difference() < 0.1:\n    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\nelse:\n    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference(), Fore.RED + \"      *** Bias Detected\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Note the results. The items in red are outside the bounds of a fair model signalling that the original model is biased and that we will need to apply bias mitigation."}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Apply Adversarial Debiasing\n\nAdversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions [1]. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.\n\nIn this cell, you will create a Debiased Model using Adversarial Debiasing and train it on the dataset.\n\nThe code below creates a new TensorFlow session and builds a machine learning model on the same data as ealier, this time with debiasing (note the `debias=True` line)."}, {"metadata": {}, "cell_type": "code", "source": "sess.close()\ntf.reset_default_graph()\nsess = tf.Session()\n\n# Learn parameters with debias set to True\ndebiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n                          unprivileged_groups = unprivileged_groups,\n                          scope_name='debiased_classifier',\n                          debias=True,\n                          sess=sess)\ndebiased_model.fit(dataset_orig_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1. Comparing Results\n\nThis step calculates and prints out the result of the bias mitigation process. "}, {"metadata": {}, "cell_type": "code", "source": "# Apply the plain model to test data\ndataset_debiasing_train = debiased_model.predict(dataset_orig_train)\ndataset_debiasing_test = debiased_model.predict(dataset_orig_test)\n\n# Metrics for the dataset from plain model (without debiasing)\ndisplay(Markdown(\"#### Plain model - without debiasing - dataset metrics\"))\nprint(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\nprint(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n\n# Metrics for the dataset from model with debiasing\ndisplay(Markdown(\"#### Model - with debiasing - dataset metrics\"))\nmetric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\nprint(Fore.RESET + \"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n\nmetric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n                                             unprivileged_groups=unprivileged_groups,\n                                             privileged_groups=privileged_groups)\n\nprint(Fore.RESET + \"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n\ndisplay(Markdown(\"#### Plain model - without debiasing - classification metrics\"))\nprint(Fore.RESET + \"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\nTPR = classified_metric_nodebiasing_test.true_positive_rate()\nTNR = classified_metric_nodebiasing_test.true_negative_rate()\nbal_acc_nodebiasing_test = 0.5*(TPR+TNR)\nprint(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n\nif 0.8 < classified_metric_nodebiasing_test.disparate_impact() < 1.2:\n    print(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\nelse:\n    print(Fore.RED + \"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n\nif  -0.1 < classified_metric_nodebiasing_test.equal_opportunity_difference() < 0.1:\n    print(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\nelse:\n    print(Fore.RED + \"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n\nif  -0.1 < classified_metric_nodebiasing_test.average_odds_difference() < 0.1:\n    print(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\nelse:\n    print(Fore.RED + \"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n\n# print(Fore.RESET + \"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n\n\ndisplay(Markdown(\"#### Model - with debiasing - classification metrics\"))\nclassified_metric_debiasing_test = ClassificationMetric(dataset_orig_test, \n                                                 dataset_debiasing_test,\n                                                 unprivileged_groups=unprivileged_groups,\n                                                 privileged_groups=privileged_groups)\nprint(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\nTPR = classified_metric_debiasing_test.true_positive_rate()\nTNR = classified_metric_debiasing_test.true_negative_rate()\nbal_acc_debiasing_test = 0.5*(TPR+TNR)\nprint(Fore.RESET + \"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\nprint(Fore.RESET + \"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact(), Fore.GREEN + \"    *** Fairness Achieved ***\")\nprint(Fore.RESET + \"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference(), Fore.GREEN + \"    *** Fairness Achieved ***\")\nprint(Fore.RESET + \"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference(), Fore.GREEN + \"    *** Fairness Achieved ***\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The lines printed in red show bias in the original model and the lines with green writing \"Fairness Achieved\" mean that the bias mitigation algorithm was successfuly applied fairness to the model and the classification metric results have moved closer to the aceptable range. In a best case scenario they will fall in the desired range. This means fairness is achieved."}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2. Visualizing Results\n\nLet's visualize the results as well. The code below takes our calculations and displays each charts comparing the original results to the results with bias mitigation applied.\n\nWe first calculate the original and mitigated results using each fairness metric (disparate impact, opportunity difference, and average odds difference):\n\nExample for disparate impact:\n\n`orig_DI = classified_metric_nodebiasing_test.disparate_impact()`\n\n`mitigated_DI = classified_metric_debiasing_test.disparate_impact()`\n\nWe then use Matplotlib, assigned to `plt` to chart our results."}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "orig_DI = classified_metric_nodebiasing_test.disparate_impact()\nmitigated_DI = classified_metric_debiasing_test.disparate_impact()\n\norig_EOD = classified_metric_nodebiasing_test.equal_opportunity_difference()\nmitigated_EOD = classified_metric_debiasing_test.equal_opportunity_difference()\n\norig_AOD = classified_metric_nodebiasing_test.average_odds_difference()\nmitigated_AOD = classified_metric_debiasing_test.average_odds_difference()\n\nN = 2\nind = np.arange(N)    # the x locations for the groups\nwidth = 0.5       # the width of the bars: can also be len(x) sequence\nxtype = ('Original', 'Mitigated')\nind = np.arange(len(xtype))\n\n# Disparate Impact\n\nvalues_DI = [orig_DI, mitigated_DI]\n# print(\"values_DI =  \", values_DI, end='\\t')\n\nplt.subplot(1,3,1)\np1_DI = plt.bar(ind, values_DI, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black') #, yerr=menStd)\nplt.xticks(ind, xtype)\nplt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\nplt.ylabel('Value')\nplt.title('Disparate Impact', fontsize=15)\nplt.ylim([-0.4, 1.2])\nplt.yticks(np.arange(-0.4, 1.3, 0.1)) \nplt.axhspan(ymin = 0.8, ymax = 1.2, color='green', alpha=0.1)\nplt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n# plt.legend(xtype)\n\n\n\n# --------------------------------------------------------------------------------------------------------\n# Equal Opprortunity Difference\n\nvalues_EOD = [orig_EOD, mitigated_EOD]\n# print(\"values_EOD = \", values_EOD, end='\\t')\n\nplt.subplot(1,3,2)\np1_EOD = plt.bar(ind, values_EOD, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black')\nplt.xticks(ind, xtype)\nplt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\nplt.ylabel('Value')\nplt.title('Equal Opportunity Difference', fontsize=15)\nplt.ylim([-0.4, 1.2])\nplt.yticks(np.arange(-0.4, 1.3, 0.1))\nplt.axhspan(ymin = -0.1, ymax = 0.1, color='green', alpha=0.1)\nplt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\n\n\n# --------------------------------------------------------------------------------------------------------\n# Average odds difference\n\nvalues_AOD = [orig_AOD, mitigated_AOD]\n# print(\"values_AOD = \", values_AOD)\n\nax = plt.subplot(1,3,3)\np1_AOD = plt.bar(ind, values_AOD, width = width, color=['darkgray', 'dodgerblue'], edgecolor='black') #, yerr=menStd)\nplt.xticks(ind, xtype)\nplt.grid(b=None, which='major', axis='y', linestyle=':', linewidth=1)\nplt.ylabel('Value')\nplt.title('Average Odds Difference', fontsize=15)\nplt.ylim(-0.4, 1.2)\nplt.yticks(np.arange(-0.4, 1.3, 0.1))\nplt.axhspan(ymin = -0.1, ymax = 0.1, color='Green', alpha=0.1)\nplt.axhspan(ymin = 0, ymax = 0, edgecolor='black', alpha=1)\nplt.subplots_adjust(top=1, bottom=0.08, left=0.10, right=2.5, hspace=0.5, wspace=0.5)\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "What does the numbers above mean? This cell demostrates the result of the bias mitigation on graphs using the difference fairness measures. \n\nThe blue bar is the mitigated result\n\nThe gray bar is the original (base case).\n\nNotice that the blue bar (with bias mitigation) is closer to the desired range and in the base case scenario, it falls in the desired range. This means that fairness is achieved."}, {"metadata": {}, "cell_type": "markdown", "source": "References:\n\n[1] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating UnwantedBiases with Adversarial Learning,\" \nAAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}